# ============================================
# Local LLM Service - Configuration
# ============================================
# 100% Offline â€¢ No External APIs Required
#
# Copy this file to .env and adjust settings as needed:
#   cp .env.example .env
# ============================================

# ============================================
# Application Settings
# ============================================
APP_NAME=Local LLM Service
VERSION=2.0.0
ENVIRONMENT=production
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=INFO

# ============================================
# CORS Settings
# ============================================
# Allowed origins (comma-separated or JSON array)
CORS_ORIGINS=["*"]

# ============================================
# Provider Settings
# ============================================
# Only local_unified is supported in this version
ENABLED_PROVIDERS=["local_provider"]

# ============================================
# Storage Directories
# ============================================
# Directory where models are stored
MODELS_DIR=./models

# Directory for application data
DATA_DIR=./data

# Cache directory for model downloads
CACHE_DIR=./models/cache

# ============================================
# Local Model Provider Configuration
# ============================================

# Context Window Size (tokens)
# Larger = more context, but more RAM/VRAM needed
# Common values: 2048, 4096, 8192, 16384
CONTEXT_SIZE=8192

# Batch Size for processing
# Larger = faster, but more memory needed
BATCH_SIZE=512

# Number of CPU threads
# Leave empty for automatic detection (recommended)
# Or set manually, e.g., N_THREADS=8
N_THREADS=8

# GPU Settings
# ============================================

# Enable GPU acceleration (CUDA)
# Set to false if you don't have a compatible GPU
USE_GPU=true

# Number of model layers to offload to GPU
# -1 = all layers on GPU (maximum speed)
# 0 = CPU only (no GPU)
# N = offload N layers (balance between speed and VRAM)
# 
# Examples:
# - RTX 3060 (12GB): -1 (all layers)
# - RTX 3060 (8GB): 32 (partial offload)
# - No GPU: 0
GPU_LAYERS=-1

# ============================================
# Generation Defaults
# ============================================

# Maximum tokens to generate
MAX_TOKENS=2048

# Temperature (0.0 = deterministic, 2.0 = very creative)
# 0.0-0.3: Focused, factual
# 0.7-1.0: Balanced (recommended)
# 1.0-2.0: Creative, diverse
DEFAULT_TEMPERATURE=0.7

# Top-p (nucleus sampling)
# 0.9 = consider top 90% probable tokens (recommended)
DEFAULT_TOP_P=0.9

# Top-k (limit to k most probable tokens)
# 40 is a good default
DEFAULT_TOP_K=40

# ============================================
# Model Download Settings
# ============================================

# HuggingFace Token (optional)
# Only needed for private/gated models
# Get your token at: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=

# Download timeout (seconds)
# Increase for slow connections or very large models
DOWNLOAD_TIMEOUT=3600

# Maximum download size (GB)
# Safety limit to prevent accidentally downloading huge models
MAX_DOWNLOAD_SIZE=50

# ============================================
# Security Settings
# ============================================

# Enable API key authentication
API_KEY_ENABLED=false

# Valid API keys (comma-separated or JSON array)
# Only used if API_KEY_ENABLED=true
API_KEYS=[]

# ============================================
# Rate Limiting
# ============================================

# Enable rate limiting
RATE_LIMIT_ENABLED=false

# Maximum requests per minute
RATE_LIMIT_REQUESTS=100

# ============================================
# Training Settings (Future Feature)
# ============================================

# Directory for fine-tuned models
TRAINING_OUTPUT_DIR=./models/fine-tuned

# Maximum training epochs
MAX_TRAINING_EPOCHS=10

# Training batch size
TRAINING_BATCH_SIZE=8

# Learning rate
LEARNING_RATE=2e-5

# ============================================
# Performance Tuning Tips
# ============================================
#
# For CPU (8-16GB RAM):
#   USE_GPU=false
#   GPU_LAYERS=0
#   CONTEXT_SIZE=2048
#   N_THREADS=8
#   Use Q4_K_M or Q2_K quantized models
#
# For GPU (8GB VRAM):
#   USE_GPU=true
#   GPU_LAYERS=32
#   CONTEXT_SIZE=4096
#   Use Q4_K_M models
#
# For GPU (12GB+ VRAM):
#   USE_GPU=true
#   GPU_LAYERS=-1
#   CONTEXT_SIZE=8192
#   Use Q4_K_M or Q5_K_M models
#
# For GPU (24GB+ VRAM):
#   USE_GPU=true
#   GPU_LAYERS=-1
#   CONTEXT_SIZE=16384
#   Can use larger models or higher quants
#
# ============================================